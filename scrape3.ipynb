{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Dirty data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def getNgrams(content, n):\n",
    "    content = content.split(' ')\n",
    "    output = []\n",
    "    for i in range(len(content)-n+1):\n",
    "        output.append(content[i:i+n])\n",
    "    return output\n",
    "\n",
    "html = urlopen('http://en.wikipedia.org/wiki/Python_(programming_language)')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "content = bs.find('div', {'id':'mw-content-text'}).get_text()\n",
    "ngrams = getNgrams(content, 2)\n",
    "print(ngrams)\n",
    "print('2-grams count is: '+str(len(ngrams)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The getNgrams function takes in an input string, splits it into a sequence of words (assuming all words are separated by spaces), and adds the n-gram (in this case, a 2- gram) that each word starts into an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def getNgrams(content, n):\n",
    "    content = re.sub('\\n|[[\\d+]]',' ',content)\n",
    "    content = bytes(content, 'UTF-8')\n",
    "    content = content.decode('ascii', 'ignore')\n",
    "    content = content.split(' ')\n",
    "    content = [word for word in content if word != '']\n",
    "    output = []\n",
    "    for i in range(len(content)-n+1):\n",
    "        output.append(content[i:i+n])\n",
    "    return output\n",
    "\n",
    "html = urlopen('http://en.wikipedia.org/wiki/Python_(programming_language)')\n",
    "bs =  BeautifulSoup(html, 'html.parser')\n",
    "content = bs.find('div', {'id':'mw-content-text'}).get_text()\n",
    "ngrams =  getNgrams(content, 2)\n",
    "print(ngrams)\n",
    "print('2-grams count is: '+str(len(ngrams)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using regular expressions to remove escape characters (such as \\n) and filtering to remove any Unicode characters, you can clean up the output somewhat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5k/1gm98l3s7l541jt9yyg0qb6w0000gn/T/ipykernel_4262/1442058878.py:14: FutureWarning: Possible nested set at position 3\n",
      "  content = re.sub('\\n|[[\\d+\\]]', ' ', content)\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "\n",
    "def cleanSentence(sentence):\n",
    "    sentence = sentence.split(' ')\n",
    "    sentence = [word.strip(string.punctuation+string.whitespace) for word in sentence]\n",
    "    sentence = [word for word in sentence if len(word) > 1 or (word.lower() == 'a' or word.lower() == 'i')]\n",
    "    return sentence\n",
    "\n",
    "def cleanInput(content):\n",
    "    content = content.upper()\n",
    "    content = re.sub('\\n|[[\\d+\\]]', ' ', content)\n",
    "    content = bytes(content, 'UTF-8')\n",
    "    content = content.decode(\"ascii\", 'ignore')\n",
    "    sentences = content.split('. ')\n",
    "    return [cleanSentence(sentence) for sentence in sentences]\n",
    "\n",
    "def getNgramsFromSentence(content, n):\n",
    "    output = []\n",
    "    for i in range(len(content)-n+1):\n",
    "        output.append(content[i:i+n])\n",
    "    return output\n",
    "\n",
    "def getNgrams(content, n):\n",
    "    content = cleanInput(content)\n",
    "    ngrams = []\n",
    "    for sentence in content:\n",
    "        ngrams.extend(getNgramsFromSentence(sentence, n))\n",
    "    return(ngrams)\n",
    "\n",
    "html = urlopen('http://en.wikipedia.org/wiki/Python_(programming_language)')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "content = bs.find('div', {'id':'mw-content-text'}).get_text()\n",
    "print(len(getNgrams(content, 2))) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def getNgrams(content, n):\n",
    "    content = cleanInput(content)\n",
    "    ngrams = Counter()\n",
    "    ngrams_list = []\n",
    "    for sentence in content:\n",
    "        newNgrams = [' '.join(ngram) for ngram in getNgramsFromSentence(sentence, n)]\n",
    "        ngrams_list.extend(newNgrams)\n",
    "        ngrams.update(newNgrams)\n",
    "    return(ngrams)\n",
    "    \n",
    "print(getNgrams(content, 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "def cleanSentence(sentence):\n",
    "    sentence = sentence.split(' ')\n",
    "    sentence = [word.strip(string.punctuation+string.whitespace) for word in sentence]\n",
    "    sentence = [word for word in sentence if len(word) > 1 or (word.lower() == 'a' or word.lower() == 'i')]\n",
    "    return sentence\n",
    "\n",
    "def cleanInput(content):\n",
    "    content = content.upper()\n",
    "    content = re.sub('\\n', ' ', content)\n",
    "    content = bytes(content, 'UTF-8')\n",
    "    content = content.decode('ascii', 'ignore')\n",
    "    sentences = content.split('. ')\n",
    "    return [cleanSentence(sentence) for sentence in sentences]\n",
    "\n",
    "def getNgramsFromSentence(content, n):\n",
    "    output = []\n",
    "    for i in range(len(content)-n+1):\n",
    "        output.append(content[i:i+n])\n",
    "    return output\n",
    "\n",
    "def getNgrams(content, n):\n",
    "    content = cleanInput(content)\n",
    "    ngrams = Counter()\n",
    "    ngrams_list = []\n",
    "    for sentence in content:\n",
    "        newNgrams = [' '.join(ngram) for ngram in getNgramsFromSentence(sentence, n)]\n",
    "        ngrams_list.extend(newNgrams)\n",
    "        ngrams.update(newNgrams)\n",
    "    return(ngrams)\n",
    "\n",
    "\n",
    "content = str(\n",
    "      urlopen('http://pythonscraping.com/files/inaugurationSpeech.txt').read(),\n",
    "              'utf-8')\n",
    "ngrams = getNgrams(content, 3)\n",
    "print(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isCommon(ngram):\n",
    "    commonWords = ['THE', 'BE', 'AND', 'OF', 'A', 'IN', 'TO', 'HAVE', 'IT', 'I', 'THAT', 'FOR', 'YOU', 'HE', 'WITH', 'ON', 'DO', 'SAY', 'THIS', 'THEY', 'IS', 'AN', 'AT', 'BUT', 'WE', 'HIS', 'FROM', 'THAT', 'NOT', 'BY', 'SHE', 'OR', 'AS', 'WHAT', 'GO', 'THEIR', 'CAN', 'WHO', 'GET', 'IF', 'WOULD', 'HER', 'ALL', 'MY', 'MAKE', 'ABOUT', 'KNOW', 'WILL', 'AS', 'UP', 'ONE', 'TIME', 'HAS', 'BEEN', 'THERE', 'YEAR', 'SO', 'THINK', 'WHEN', 'WHICH', 'THEM', 'SOME', 'ME', 'PEOPLE', 'TAKE', 'OUT', 'INTO', 'JUST', 'SEE', 'HIM', 'YOUR', 'COME', 'COULD', 'NOW', 'THAN', 'LIKE', 'OTHER', 'HOW', 'THEN', 'ITS', 'OUR', 'TWO', 'MORE', 'THESE', 'WANT', 'WAY', 'LOOK', 'FIRST', 'ALSO', 'NEW', 'BECAUSE', 'DAY', 'MORE', 'USE', 'NO', 'MAN', 'FIND', 'HERE', 'THING', 'GIVE', 'MANY', 'WELL']\n",
    "    for word in ngram:\n",
    "        if word in commonWords:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def getNgramsFromSentence(content, n):\n",
    "    output = []\n",
    "    for i in range(len(content)-n+1):\n",
    "        if not isCommon(content[i:i+n]):\n",
    "            output.append(content[i:i+n])\n",
    "    return output\n",
    "\n",
    "ngrams = getNgrams(content, 3)\n",
    "print(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IF THERE IS ONE MEASURE BETTER CALCULATED THAN ANOTHER TO PRODUCE THAT STATE OF THINGS SO MUCH DEPRECATED BY ALL TRUE REPUBLICANS, BY WHICH THE RICH ARE DAILY ADDING TO THEIR HOARDS AND THE POOR SINKING DEEPER INTO PENURY, IT IS AN EXCLUSIVE METALLIC CURRENCY\n",
      "\n",
      "SUCH A ONE WAS AFFORDED BY THE EXECUTIVE DEPARTMENT CONSTITUTED BY THE CONSTITUTION\n",
      "\n",
      "THE GENERAL GOVERNMENT HAS SEIZED UPON NONE OF THE RESERVED RIGHTS OF THE STATES\n",
      "\n",
      "CALLED FROM A RETIREMENT WHICH I HAD SUPPOSED WAS TO CONTINUE FOR THE RESIDUE OF MY LIFE TO FILL THE CHIEF EXECUTIVE OFFICE OF THIS GREAT AND FREE NATION, I APPEAR BEFORE YOU, FELLOW-CITIZENS, TO TAKE THE OATHS WHICH THE CONSTITUTION PRESCRIBES AS A NECESSARY QUALIFICATION FOR THE PERFORMANCE OF ITS DUTIES; AND IN OBEDIENCE TO A CUSTOM COEVAL WITH OUR GOVERNMENT AND WHAT I BELIEVE TO BE YOUR EXPECTATIONS I PROCEED TO PRESENT TO YOU A SUMMARY OF THE PRINCIPLES WHICH WILL GOVERN ME IN THE DISCHARGE OF THE DUTIES WHICH I SHALL BE CALLED UPON TO PERFORM.\n",
      "\n",
      "IT WAS THE REMARK OF A ROMAN CONSUL IN AN EARLY PERIOD OF THAT CELEBRATED REPUBLIC THAT A MOST STRIKING CONTRAST WAS OBSERVABLE IN THE CONDUCT OF CANDIDATES FOR OFFICES OF POWER AND TRUST BEFORE AND AFTER OBTAINING THEM, THEY SELDOM CARRYING OUT IN THE LATTER CASE THE PLEDGES AND PROMISES MADE IN THE FORMER\n",
      "\n",
      "HOWEVER MUCH THE WORLD MAY HAVE IMPROVED IN MANY RESPECTS IN THE LAPSE OF UPWARD OF TWO THOUSAND YEARS SINCE THE REMARK WAS MADE BY THE VIRTUOUS AND INDIGNANT ROMAN, I FEAR THAT A STRICT EXAMINATION OF THE ANNALS OF SOME OF THE MODERN ELECTIVE GOVERNMENTS WOULD DEVELOP SIMILAR INSTANCES OF VIOLATED CONFIDENCE.\n",
      "\n",
      "ALTHOUGH THE FIAT OF THE PEOPLE HAS GONE FORTH PROCLAIMING ME THE CHIEF MAGISTRATE OF THIS GLORIOUS UNION, NOTHING UPON THEIR PART REMAINING TO BE DONE, IT MAY BE THOUGHT THAT A MOTIVE MAY EXIST TO KEEP UP THE DELUSION UNDER WHICH THEY MAY BE SUPPOSED TO HAVE ACTED IN RELATION TO MY PRINCIPLES AND OPINIONS; AND PERHAPS THERE MAY BE SOME IN THIS ASSEMBLY WHO HAVE COME HERE EITHER PREPARED TO CONDEMN THOSE I SHALL NOW DELIVER, OR, APPROVING THEM, TO DOUBT THE SINCERITY WITH WHICH THEY ARE NOW UTTERED\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getFirstSentenceContaining(ngram, content):\n",
    "    #print(ngram)\n",
    "    sentences = content.upper().split(\". \")\n",
    "    for sentence in sentences: \n",
    "        if ngram in sentence:\n",
    "            return sentence+'\\n'\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "print(getFirstSentenceContaining('EXCLUSIVE METALLIC CURRENCY', content))\n",
    "print(getFirstSentenceContaining('EXECUTIVE DEPARTMENT', content))\n",
    "print(getFirstSentenceContaining('GENERAL GOVERNMENT', content))\n",
    "print(getFirstSentenceContaining('CALLED UPON', content))\n",
    "print(getFirstSentenceContaining('CHIEF MAGISTRATE', content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have amply maintained their own sphere , or classed with the head and tolerant and fostering a union that 'most' of things so extensive Confederacy , it will . Men blinded by the remembrance of the acknowledged to the very remote period so distinctly drawn as it is the object . The presses in the influence of the personal liberty . It is the spoils and , to keep down a full of feeling , within the field , suggested by my opinion may be observed . Upward of the means in the character of a vestige of these patriots\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from random import randint\n",
    "\n",
    "def wordListSum(wordList):\n",
    "    sum = 0\n",
    "    for word, value in wordList.items():\n",
    "        sum += value\n",
    "    return sum\n",
    "\n",
    "def retrieveRandomWord(wordList):\n",
    "    randIndex = randint(1, wordListSum(wordList))\n",
    "    for word, value in wordList.items():\n",
    "        randIndex -= value\n",
    "        if randIndex <= 0:\n",
    "            return word\n",
    "\n",
    "def buildWordDict(text):\n",
    "    # Remove newlines and quotes\n",
    "    text = text.replace('\\n', ' ');\n",
    "    text = text.replace('\"', '');\n",
    "\n",
    "    # Make sure punctuation marks are treated as their own \"words,\"\n",
    "    # so that they will be included in the Markov chain\n",
    "    punctuation = [',','.',';',':']\n",
    "    for symbol in punctuation:\n",
    "        text = text.replace(symbol, ' {} '.format(symbol));\n",
    "\n",
    "    words = text.split(' ')\n",
    "    # Filter out empty words\n",
    "    words = [word for word in words if word != '']\n",
    "\n",
    "    wordDict = {}\n",
    "    for i in range(1, len(words)):\n",
    "        if words[i-1] not in wordDict:\n",
    "                # Create a new dictionary for this word\n",
    "            wordDict[words[i-1]] = {}\n",
    "        if words[i] not in wordDict[words[i-1]]:\n",
    "            wordDict[words[i-1]][words[i]] = 0\n",
    "        wordDict[words[i-1]][words[i]] += 1\n",
    "    return wordDict\n",
    "\n",
    "text = str(urlopen('http://pythonscraping.com/files/inaugurationSpeech.txt')\n",
    "          .read(), 'utf-8')\n",
    "wordDict = buildWordDict(text)\n",
    "\n",
    "#Generate a Markov chain of length 100\n",
    "length = 100\n",
    "chain = ['I']\n",
    "for i in range(0, length):\n",
    "    newWord = retrieveRandomWord(wordDict[chain[-1]])\n",
    "    chain.append(newWord)\n",
    "\n",
    "print(' '.join(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "conn = pymysql.connect(\n",
    "    host='127.0.0.1', \n",
    "    unix_socket='/tmp/mysql.sock', \n",
    "    user='root', \n",
    "    passwd=input('Enter your password'), \n",
    "    db='mysql', \n",
    "    charset='utf8')\n",
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute('USE wikipedia')\n",
    "\n",
    "def getUrl(pageId):\n",
    "    cur.execute('SELECT url FROM pages WHERE id = %s', (int(pageId)))\n",
    "    return cur.fetchone()[0]\n",
    "\n",
    "def getLinks(fromPageId):\n",
    "    cur.execute('SELECT toPageId FROM links WHERE fromPageId = %s', (int(fromPageId)))\n",
    "    if cur.rowcount == 0:\n",
    "        return []\n",
    "    return [x[0] for x in cur.fetchall()]\n",
    "\n",
    "def searchBreadth(targetPageId, paths=[[1]]):\n",
    "    newPaths = []\n",
    "    for path in paths:\n",
    "        links = getLinks(path[-1])\n",
    "        for link in links:\n",
    "            if link == targetPageId:\n",
    "                return path + [link]\n",
    "            else:\n",
    "                newPaths.append(path+[link])\n",
    "    return searchBreadth(targetPageId, newPaths)\n",
    "                \n",
    "nodes = getLinks(1)\n",
    "targetPageId = 28624\n",
    "pageIds = searchBreadth(targetPageId)\n",
    "for pageId in pageIds:\n",
    "    print(getUrl(pageId))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling through Forms and Logins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there,  !\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "params = {'firstname': 'Kilua', 'lastname': 'Gon'}\n",
    "r = requests.post(\"http://pythonscraping.com/pages/processing.php\", data=params)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "params = {'email_addr': 'ryan.e.mitchell@gmail.com'}\n",
    "r = requests.post(\"http://post.oreilly.com/client/o/oreilly/forms/quicksignup.cgi\",\n",
    "                   data=params)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "files = {'uploadFile': open('files/Python-logo.png', 'rb')}\n",
    "r = requests.post('http://pythonscraping.com/pages/processing2.php', files=files)\n",
    "print(r.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uploading a file or an image to a website using the post method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookie is set to:\n",
      "{}\n",
      "Going to profile page...\n",
      "You're not logged into the site!<br>Visit <a href=\"login.html\">the login page</a> to log in\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "params = {'username': 'KILUA', 'password': 'password'}\n",
    "r = requests.post('http://pythonscraping.com/pages/cookies/welcome.php', params)\n",
    "print('Cookie is set to:')\n",
    "print(r.cookies.get_dict())\n",
    "print('Going to profile page...')\n",
    "r = requests.get('http://pythonscraping.com/pages/cookies/profile.php', \n",
    "                 cookies=r.cookies)\n",
    "print(r.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cookies keep track of who is logged in and what they have access to. The requests library has a built-in cookie jar that can be used to store cookies and send them with subsequent requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookie is set to:\n",
      "{}\n",
      "Going to profile page...\n",
      "You're not logged into the site!<br>Visit <a href=\"login.html\">the login page</a> to log in\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "params = {'username': 'Kilua', 'password': 'password'}\n",
    "s = session.post('http://pythonscraping.com/pages/cookies/welcome.php', params)\n",
    "print(\"Cookie is set to:\")\n",
    "print(s.cookies.get_dict())\n",
    "print('Going to profile page...')\n",
    "s = session.get('http://pythonscraping.com/pages/cookies/profile.php')\n",
    "print(s.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>Hello ryan.</p><p>You entered password as your password.</p>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from requests.auth import AuthBase\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "auth = HTTPBasicAuth('ryan', 'password')\n",
    "r = requests.post(\n",
    "    url='http://pythonscraping.com/pages/auth/login.php', auth=auth)\n",
    "print(r.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JavaScript, Selenium, and PhantomJS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5k/1gm98l3s7l541jt9yyg0qb6w0000gn/T/ipykernel_45679/3082658365.py:8: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is some important text you want to retrieve!\n",
      "A button to click!\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "driver = webdriver.Chrome(\n",
    "    executable_path='drivers/chromedriver', \n",
    "    options=chrome_options)\n",
    "driver.get('http://pythonscraping.com/pages/javascript/ajaxDemo.html')\n",
    "time.sleep(3)\n",
    "print(driver.find_element(By.ID, 'content').text)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5k/1gm98l3s7l541jt9yyg0qb6w0000gn/T/ipykernel_45679/1532427364.py:8: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is some important text you want to retrieve!\n",
      "A button to click!\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "driver = webdriver.Chrome(\n",
    "    executable_path='drivers/chromedriver',\n",
    "    options=chrome_options)\n",
    "\n",
    "driver.get('http://pythonscraping.com/pages/javascript/ajaxDemo.html')\n",
    "try:\n",
    "    element = WebDriverWait(driver, 10).until(\n",
    "                       EC.presence_of_element_located((By.ID, 'loadedButton')))\n",
    "finally:\n",
    "    print(driver.find_element(By.ID, 'content').text)\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5k/1gm98l3s7l541jt9yyg0qb6w0000gn/T/ipykernel_45679/1748541290.py:22: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timing out after 10 seconds and returning\n",
      "<html><head>\n",
      "<title>The Destination Page!</title>\n",
      "\n",
      "</head>\n",
      "<body>\n",
      "This is the page you are looking for!\n",
      "\n",
      "</body></html>\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.remote.webelement import WebElement\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "import time\n",
    "\n",
    "def waitForLoad(driver):\n",
    "    elem = driver.find_element(By.TAG_NAME,\"html\")\n",
    "    count = 0\n",
    "    while True:\n",
    "        count += 1\n",
    "        if count > 20:\n",
    "            print(\"Timing out after 10 seconds and returning\")\n",
    "            return\n",
    "        time.sleep(.5)\n",
    "        try:\n",
    "            elem == driver.find_element(By.TAG_NAME ,\"html\")\n",
    "        except StaleElementReferenceException:\n",
    "            return\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "driver = webdriver.Chrome(\n",
    "    executable_path='drivers/chromedriver',\n",
    "    options=chrome_options)\n",
    "driver.get(\"http://pythonscraping.com/pages/javascript/redirectDemo1.html\")\n",
    "waitForLoad(driver)\n",
    "print(driver.page_source)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5k/1gm98l3s7l541jt9yyg0qb6w0000gn/T/ipykernel_64578/3300778754.py:9: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the page you are looking for!\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "driver = webdriver.Chrome(\n",
    "    executable_path='drivers/chromedriver', \n",
    "    options=chrome_options)\n",
    "driver.get('http://pythonscraping.com/pages/javascript/redirectDemo1.html')\n",
    "try:\n",
    "    bodyElement = WebDriverWait(driver, 15).until(EC.presence_of_element_located(\n",
    "        (By.XPATH, '//body[contains(text(), \"This is the page you are looking for!\")]')))\n",
    "    print(bodyElement.text)\n",
    "except TimeoutException:\n",
    "    print('Did not find the element')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PhantomJS is a headless browser that can be used to render JavaScript and execute AJAX requests. It is a good alternative to Selenium when you don’t need to interact with the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'selenium.webdriver' has no attribute 'PhantomJS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5k/1gm98l3s7l541jt9yyg0qb6w0000gn/T/ipykernel_45679/643156163.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPhantomJS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecutable_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/Users/oscar/desktop/Android-kotlin/DataScience/phantomjs-2.1.1-macosx/bin/phantomjs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http://pythonscraping.com/pages/javascript/redirectDemo1.html'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mwaitForLoad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'selenium.webdriver' has no attribute 'PhantomJS'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.remote.webelement import WebElement\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "def waitForLoad(driver):\n",
    "    elem = driver.find_element(By.TAG_NAME, \"html\") \n",
    "    count = 0\n",
    "    while True:\n",
    "        count += 1\n",
    "        if count > 20:\n",
    "            print('Timing out after 10 seconds and returning')\n",
    "            return\n",
    "\n",
    "        time.sleep(.5) \n",
    "        try:\n",
    "            elem == driver.find_element(By.TAG_NAME,'html') \n",
    "        except StaleElementReferenceException:\n",
    "            return\n",
    "\n",
    "driver = webdriver.PhantomJS(executable_path='/Users/oscar/desktop/Android-kotlin/DataScience/phantomjs-2.1.1-macosx/bin/phantomjs') \n",
    "driver.get('http://pythonscraping.com/pages/javascript/redirectDemo1.html') \n",
    "waitForLoad(driver)\n",
    "print(driver.page_source)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error is likely caused by the fact that the PhantomJS project is no longer maintained and the library is not being updated. This means that it is not compatible with the latest version of Selenium or web browsers. As a result, the \"PhantomJS\" class is no longer available in the webdriver module.\n",
    "One alternative solution is to use the headless chrome browser by using \"Chrome\" or \"Firefox\" webdrivers with the option '--headless'\n",
    "Another alternative solution is to use other headless browser like \"puppeteer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyppeteer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using puppeteer \n",
    "import asyncio\n",
    "from pyppeteer import launch\n",
    "\n",
    "async def wait_for_load(page):\n",
    "    await page.waitForSelector('html')\n",
    "\n",
    "async def main():\n",
    "    browser = await launch()\n",
    "    page = await browser.newPage()\n",
    "    await page.goto('http://pythonscraping.com/pages/javascript/redirectDemo1.html')\n",
    "    await wait_for_load(page)\n",
    "    page_source = await page.content()\n",
    "    print(page_source)\n",
    "    await browser.close()\n",
    "\n",
    "asyncio.get_event_loop().run_until_complete(main())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, the pyppeteer library is used to interact with a headless version of Chromium (Chrome's open-source core) via the launch function. The newPage function is used to create a new page in the browser, and goto method is used to navigate to the URL. The waitForSelector function is used to wait for the element with the tag \"html\" to be loaded. Finally, the page source is obtained with the content() function and printed to the console. Once the script is done, the browser instance is closed using the close() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium.common.exceptions import TimeoutException\n",
    "    \n",
    "driver = webdriver.PhantomJS(executable_path=\n",
    "        '/Users/oscar/desktop/Android-kotlin/DataScience/phantomjs-2.1.1-macosx/bin/phantomjs')\n",
    "\n",
    "driver.get('http://pythonscraping.com/pages/javascript/redirectDemo1.html')\n",
    "\n",
    "try:\n",
    "    bodyElement = WebDriverWait(driver, 15).until(EC.presence_of_element_located(\n",
    "            (By.XPATH, '//body[contains(text(),\n",
    "            \"This is the page you are looking for!)]\")))\n",
    "    print(bodyElement.text) \n",
    "except TimeoutException:\n",
    "    print('Did not find the element')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PhantomJs is no longer supported "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from pyppeteer import launch\n",
    "\n",
    "async def main():\n",
    "    browser = await launch()\n",
    "    page = await browser.newPage()\n",
    "    await page.goto('http://pythonscraping.com/pages/javascript/redirectDemo1.html')\n",
    "    try:\n",
    "        body_element = await page.waitForXPath('//body[contains(text(),\"This is the page you are looking for!)]', {'timeout': 15000})\n",
    "        body_text = await page.evaluate('(element) => element.textContent', body_element)\n",
    "        print(body_text)\n",
    "    except TimeoutError:\n",
    "        print('Did not find the element')\n",
    "    await browser.close()\n",
    "\n",
    "asyncio.get_event_loop().run_until_complete(main())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, the pyppeteer library is used to interact with a headless version of Chromium (Chrome's open-source core) via the launch function. The newPage function is used to create a new page in the browser, and goto method is used to navigate to the URL. The waitForXPath function is used to wait for the element with the xpath '//body[contains(text(),\"This is the page you are looking for!)]' to be loaded and the 'evaluate' method is used to extract the text of the body element. Finally, the text is printed to the console. If the element is not found, a 'TimeoutError' will be raised and the message 'Did not find the element' will be printed. Once the script is done, the browser instance is closed using the close() function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b5221f7fd568601ffb692ded55cc6a8f9f720c6422993e13b3964f3c2d5ea0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
